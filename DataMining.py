# -*- coding: utf-8 -*-
"""Copy of Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGwNEVGiyCR3RF4mHtJG3h6TTSEm1RNz
"""

import pandas as pd

# Create a dictionary from the DataFrame
def create_dict_from_df(df):
    # Split the 'items' string by commas and strip spaces, then convert to a list
    df['items'] = df['items'].str.split(',').apply(lambda x: [item.strip() for item in x])
    # Create a dictionary with 'TiD' as keys and 'items' as values
    transaction_dict = pd.Series(df['items'].values, index=df['TiD']).to_dict()
    return transaction_dict

def eclat(horizontal_data, min_support):
    # Step 1: Convert horizontal data to vertical format
    vertical_data = {}
    for tid, items in horizontal_data.items():
        for item in items:
            if item in vertical_data:
                vertical_data[item].add(tid)
            else:
                vertical_data[item] = {tid}

    # Step 2: Find all frequent itemsets
    def find_frequent_itemsets(vertical_data, min_support):
        frequent_itemsets = {}
        for item, tids in vertical_data.items():
            if len(tids) >= min_support:
                frequent_itemsets[frozenset([item])] = tids
        return frequent_itemsets

    # Helper function to get the TID set for an itemset
    def get_tid_set(itemset, vertical_data):
        tid_sets = [vertical_data[item] for item in itemset]
        return set.intersection(*tid_sets)

    # Step 3: Generate new candidate itemsets from existing frequent itemsets
    def generate_new_candidates(frequent_itemsets, k):
        new_candidates = {}
        itemsets = list(frequent_itemsets.keys())
        for i in range(len(itemsets)):
            for j in range(i+1, len(itemsets)):
                # Join sets if they have k-1 items in common
                itemset1, itemset2 = itemsets[i], itemsets[j]
                if len(itemset1.union(itemset2)) == k:
                    new_candidates[itemset1.union(itemset2)] = get_tid_set(itemset1.union(itemset2), vertical_data)
        return new_candidates

    # Step 4: Iterate through increasing itemset sizes until no more frequent itemsets are found
    k = 1
    all_frequent_itemsets = {}
    current_frequent_itemsets = find_frequent_itemsets(vertical_data, min_support)
    while current_frequent_itemsets:
        all_frequent_itemsets.update(current_frequent_itemsets)
        k += 1
        candidate_itemsets = generate_new_candidates(current_frequent_itemsets, k)
        current_frequent_itemsets = {itemset: tids for itemset, tids in candidate_itemsets.items() if len(tids) >= min_support}

    return all_frequent_itemsets

def generate_association_rules(frequent_itemsets, min_confidence):
    association_rules = []
    for itemset in frequent_itemsets.keys():
        if len(itemset) > 1:
            for item in itemset:
                antecedent = frozenset([item])
                consequent = itemset.difference(antecedent)
                confidence = len(frequent_itemsets[itemset]) / len(frequent_itemsets[antecedent])
                if confidence >= min_confidence:
                    association_rules.append((antecedent, consequent, confidence))
                if len(itemset) > 2:
                  consequent = frozenset([item])
                  antecedent = itemset.difference(consequent)
                  confidence = len(frequent_itemsets[itemset]) / len(frequent_itemsets[consequent])
                  if confidence >= min_confidence:
                    association_rules.append((antecedent, consequent, confidence))
    return association_rules

def calculate_lift(itemset, antecedent, consequent, frequent_itemsets):
    support_itemset = len(frequent_itemsets[itemset])
    support_antecedent = len(frequent_itemsets[antecedent])
    support_consequent = len(frequent_itemsets[consequent])

    if df.columns[0] == "TiD":
        lift = (len(horizontal_data) * support_itemset) / (support_antecedent * support_consequent)
    elif df.columns[0] == "items":
      all_numbers = []
      for key in horizontal_data:
        numbers = key.split(',')
        numbers = [int(num) for num in numbers]
        all_numbers.extend(numbers)
        max_number = max(all_numbers)
      lift = (max_number * support_itemset) / (support_antecedent * support_consequent)

    return lift

df = pd.read_excel('Horizontal_Format.xlsx')


horizontal_data = create_dict_from_df(df)
min_support = input("Please enter the minimum suppurt: ")
min_support = float(min_support)
if df.columns[0]=="TiD":
  frequent_itemsets = eclat(horizontal_data, min_support)
min_confidence1 = -1
min_confidence2 = input("Please enter the minimum confidence: ")
min_confidence2 = float(min_confidence2)

# Execute the ECLAT algorithm

for itemset, tids in frequent_itemsets.items():
    print(f"Itemset: {itemset}, Support: {len(tids)}")
# Generate association rules
all_association_rules = generate_association_rules(frequent_itemsets, min_confidence1)
strong_association_rules = generate_association_rules(frequent_itemsets, min_confidence2)
# Print association rules and calculate lift
print("----------------------------------------------------")
print("All Association Rules")
for antecedent, consequent, confidence in all_association_rules:
    print(f"Rule: {antecedent} -> {consequent}, Confidence: {confidence}")
    lift = calculate_lift(antecedent.union(consequent), antecedent, consequent, frequent_itemsets)
    print(f"Lift: {lift}")
    if(lift > 1):
      print("dependent, +ve correlated")
    elif(lift < 1):
      print("dependent, -ve correlated")
    else:
      print("Independent")
print("----------------------------------------------------")
print("Strong Association Rules")
for antecedent, consequent, confidence in strong_association_rules:
    print(f"Rule: {antecedent} -> {consequent}, Confidence: {confidence}")
    lift = calculate_lift(antecedent.union(consequent), antecedent, consequent, frequent_itemsets)
    print(f"Lift: {lift}")
    if(lift > 1):
      print("dependent, +ve correlated")
    elif(lift < 1):
      print("dependent, -ve correlated")
    else:
      print("Independent")